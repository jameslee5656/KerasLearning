{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os\n",
    "import tarfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url='http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "filepath='data/aclImdb_v1.tar.gz'\n",
    "if not os.path.isfile(filepath):\n",
    "    result=urllib.request.urlretrieve(url,filepath)\n",
    "    print('download:',result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('data/aclImdb'):\n",
    "    tfile = tarfile.open('data/aclImdb_v1.tar.gz','r:gz')\n",
    "    result=tfile.extractall('data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.tensorflow_backend._get_available_gpus()\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_files(filetype):\n",
    "    path = \"data/aclImbd\"\n",
    "    file_list = []\n",
    "    \n",
    "    positive_path = path + '/' + filetype + '/pos/'\n",
    "    for f in os.listdir(positive_path):\n",
    "        file_list+=[positive_path+f]\n",
    "        \n",
    "    negative_path = path + '/' + filetype + '/neg/'\n",
    "    for f in os.listdir(negative_path):\n",
    "        file_list+=[negative_path+f]\n",
    "    \n",
    "    print('read',filetype, 'files:',len(file_list))\n",
    "    \n",
    "    all_labels = ([1] * 12500 + [0] * 12500)\n",
    "    \n",
    "    all_texts = []\n",
    "    for fi in file_list:\n",
    "        with open(fi, encoding = 'utf8') as file_input:\n",
    "            all_texts += [rm_tags(\" \".join(file_input.readlines()))]\n",
    "    return all_labels, all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read train files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_train,train_text=read_files('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read test files: 25000\n"
     ]
    }
   ],
   "source": [
    "y_test,test_text=read_files('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's been a spate of recent surfing movies that I seem to haphazardly run across without advance warning. I caught this treasure on digital cable this week and what a pleasant surprise it was! The focus is on the pioneers of big wave surfing from the 60's Greg Noll to our current Laird Hamilton, from Waimea Bay to Mavericks to Jaws. Hell, I could watch a movie just about Laird Hamilton - one of this generation's great athletes - so the rest is just gravy. There's loads of good surfing mixed in with interviews of past and present surfing stars, in pleasant, relaxed and unpretentious fashion. Of all the surfing movies I've seen this tells the big-wave story the best, and I think it's my favorite. Enjoy!\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer(num_words=3800)\n",
    "token.fit_on_texts(train_text)\n",
    "print(token.document_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(token.word_index)\n",
    "x_train_seq = token.texts_to_sequences(train_text)\n",
    "x_test_seq = token.texts_to_sequences(test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There's been a spate of recent surfing movies that I seem to haphazardly run across without advance warning. I caught this treasure on digital cable this week and what a pleasant surprise it was! The focus is on the pioneers of big wave surfing from the 60's Greg Noll to our current Laird Hamilton, from Waimea Bay to Mavericks to Jaws. Hell, I could watch a movie just about Laird Hamilton - one of this generation's great athletes - so the rest is just gravy. There's loads of good surfing mixed in with interviews of past and present surfing stars, in pleasant, relaxed and unpretentious fashion. Of all the surfing movies I've seen this tells the big-wave story the best, and I think it's my favorite. Enjoy!\n"
     ]
    }
   ],
   "source": [
    "print(train_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[221, 73, 3, 4, 1132, 98, 11, 9, 302, 5, 517, 633, 205, 1711, 9, 1056, 10, 19, 1874, 10, 1264, 2, 47, 3, 860, 8, 12, 1, 1144, 6, 19, 1, 4, 190, 35, 1, 5, 259, 35, 5, 5, 604, 9, 96, 102, 3, 16, 39, 40, 27, 4, 10, 83, 34, 1, 356, 6, 39, 221, 4, 48, 1845, 7, 15, 4, 497, 2, 975, 379, 7, 2, 1596, 4, 28, 1, 98, 203, 106, 10, 712, 1, 190, 61, 1, 114, 2, 9, 100, 41, 57, 510, 353]\n"
     ]
    }
   ],
   "source": [
    "print(x_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = sequence.pad_sequences(x_train_seq, maxlen=380)\n",
    "x_test = sequence.pad_sequences(x_test_seq, maxlen=380)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before pad_sequences length= 101\n",
      "[221, 73, 3, 4, 1132, 98, 11, 9, 302, 5, 517, 633, 205, 1711, 9, 1056, 10, 2523, 19, 3534, 1874, 10, 1264, 2, 47, 3, 2203, 860, 8, 12, 1, 1144, 6, 19, 1, 4, 190, 2861, 35, 1, 3127, 5, 259, 2023, 35, 5, 5, 604, 9, 96, 102, 3, 16, 39, 40, 27, 4, 10, 83, 34, 1, 356, 6, 39, 221, 4, 48, 1845, 7, 15, 3007, 4, 497, 2, 975, 379, 7, 2203, 2, 1596, 4, 28, 1, 98, 203, 106, 10, 712, 1, 190, 2861, 61, 1, 114, 2, 9, 100, 41, 57, 510, 353]\n"
     ]
    }
   ],
   "source": [
    "print('before pad_sequences length=', len(x_train_seq[0]))\n",
    "print(x_train_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after pad_sequences length= 380\n",
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0  221\n",
      "   73    3    4 1132   98   11    9  302    5  517  633  205 1711    9\n",
      " 1056   10 2523   19 3534 1874   10 1264    2   47    3 2203  860    8\n",
      "   12    1 1144    6   19    1    4  190 2861   35    1 3127    5  259\n",
      " 2023   35    5    5  604    9   96  102    3   16   39   40   27    4\n",
      "   10   83   34    1  356    6   39  221    4   48 1845    7   15 3007\n",
      "    4  497    2  975  379    7 2203    2 1596    4   28    1   98  203\n",
      "  106   10  712    1  190 2861   61    1  114    2    9  100   41   57\n",
      "  510  353]\n"
     ]
    }
   ],
   "source": [
    "print('after pad_sequences length=', len(x_train[0]))\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense ,Dropout, Activation, Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#Add embedding layer\n",
    "model.add(Embedding(output_dim=32,\n",
    "                    input_dim=3800,\n",
    "                    input_length=380))\n",
    "model.add(Dropout(0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(units=256,\n",
    "                activation='relu'))\n",
    "model.add(Dropout(0.35))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 380, 32)           121600    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 380, 32)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12160)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               3113216   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,235,073\n",
      "Trainable params: 3,235,073\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.add(Dense(units=1,\n",
    "                activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      " - 2s - loss: 0.4677 - acc: 0.7633 - val_loss: 0.5338 - val_acc: 0.7504\n",
      "Epoch 2/10\n",
      " - 2s - loss: 0.1876 - acc: 0.9279 - val_loss: 0.5227 - val_acc: 0.7794\n",
      "Epoch 3/10\n",
      " - 2s - loss: 0.0695 - acc: 0.9777 - val_loss: 0.5323 - val_acc: 0.8234\n",
      "Epoch 4/10\n",
      " - 2s - loss: 0.0256 - acc: 0.9933 - val_loss: 0.8743 - val_acc: 0.7776\n",
      "Epoch 5/10\n",
      " - 2s - loss: 0.0145 - acc: 0.9957 - val_loss: 0.8058 - val_acc: 0.8158\n",
      "Epoch 6/10\n",
      " - 2s - loss: 0.0121 - acc: 0.9961 - val_loss: 0.9353 - val_acc: 0.8064\n",
      "Epoch 7/10\n",
      " - 2s - loss: 0.0105 - acc: 0.9970 - val_loss: 1.0928 - val_acc: 0.7884\n",
      "Epoch 8/10\n",
      " - 2s - loss: 0.0114 - acc: 0.9962 - val_loss: 0.9054 - val_acc: 0.8284\n",
      "Epoch 9/10\n",
      " - 2s - loss: 0.0176 - acc: 0.9936 - val_loss: 1.1501 - val_acc: 0.7840\n",
      "Epoch 10/10\n",
      " - 2s - loss: 0.0169 - acc: 0.9939 - val_loss: 1.3050 - val_acc: 0.7660\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "train_history = model.fit(x_train, y_train, batch_size=100,\n",
    "                         epochs=10,verbose=2,\n",
    "                         validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000/25000 [==============================] - 1s 44us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.84176"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "scores[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [1],\n",
       "       [1]], dtype=int32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict=model.predict_classes(x_test)\n",
    "predict[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_classes=predict.reshape(-1)\n",
    "predict_classes[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "SentimentDict={1:'Positive',0:'Negative'}\n",
    "def display_test_Sentiment(i):\n",
    "    print(test_text[i])\n",
    "    print('label real value:', SentimentDict[y_test[i]],\n",
    "          'predicted value:', SentimentDict[predict_classes[i]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The idea is not original... If you have seen such kind of story before, you would know what the ending would come out after watching for the first twenty minutes... the script, the positioning of the actors and the screening is too obvious... If you haven't seen such story before, it is definitely a good experience, you will enjoy the twist at the end...don't forget to watch it again after you know the \"truth\", you will even more enjoy the plots... Even though I have a right guess at the very beginning, I still couldn't help stick on my seat till the end...Conclusion: A must see!! This one from Korea is better than any recent movies of the genre from Japan...forget Hollywood!Don't miss it!!\n",
      "label real value: Positive predicted value: Negative\n"
     ]
    }
   ],
   "source": [
    "display_test_Sentiment(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text='''\n",
    "Glass is truly a cinematic jewel, which closes the delicate and complex analysis begun by \n",
    "the two previous films.A journey to discover the inner strength (or weakness) that each of \n",
    "us has as a human being.Shyamalan directs this film, interpreted in an excellent way by all\n",
    "the protagonists, without smearing and will surely disappoint all those people who will go \n",
    "to the cinema with the hope of seeing a cinecomics or an action movie.Glass tackles the \n",
    "mystery of souls and does so in an intelligent and almost delicate way. A truly excellent \n",
    "product, Cinema for high and sensitive minds. Show to reflect on our existence using \n",
    "archetypes and almost alchemical references. Suffice it to think of the continuous and \n",
    "stressed reference to number 3 or the events of the Ulysses of the Odyssey of which the \n",
    "film is full of quotations from the attentive spectator.Bravo, Shyamalan. Thank you so much \n",
    "for this wonderful spiritual journey veered only apparently in the world of comics. Standing \n",
    "Ovation.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[6, 367, 3, 1356, 59, 1, 2, 1311, 30, 1, 103, 956, 104, 3, 1308, 5, 1969, 1, 38, 11, 253, 4, 174, 43, 13, 3, 402, 108, 10, 18, 7, 31, 317, 92, 30, 28, 1, 205, 2, 76, 1340, 28, 144, 80, 33, 76, 136, 5, 1, 434, 15, 1, 436, 4, 315, 3, 38, 31, 202, 16, 1, 731, 4, 2, 123, 34, 7, 31, 1086, 2, 216, 92, 3, 367, 317, 434, 14, 308, 2, 119, 5, 19, 259, 768, 2, 216, 8, 5, 100, 4, 1, 2, 5, 608, 339, 38, 1, 684, 4, 1, 4, 1, 4, 59, 1, 18, 6, 364, 4, 35, 1, 1294, 21, 34, 72, 14, 10, 385, 1308, 60, 682, 7, 1, 178, 4]]\n"
     ]
    }
   ],
   "source": [
    "input_seq = token.texts_to_sequences([input_text])\n",
    "print(input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pad_input_seq = sequence.pad_sequences(input_seq, maxlen=100)\n",
    "len(pad_input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_result = model.predict_classes(pad_input_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]], dtype=int32)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Positive'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentDict[predict_result[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_review(input_text):\n",
    "    input_seq = token.texts_to_sequences([input_text])\n",
    "    pad_input_seq = sequence.pad_sequences(input_seq,maxlen=380)\n",
    "    predict_result=model.predict_classes(pad_input_seq)\n",
    "    print(SentimentDict[predict_result[0][0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive\n"
     ]
    }
   ],
   "source": [
    "predict_review(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
